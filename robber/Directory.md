目录规划
========

收集模块
--------

* collect 

  * 爬虫开始爬虫或爬虫运行中进行收集URL
  * 支持filter过滤器.
  * 分3个类别
  * threading
    * 模块开始时只运行一次 有默认值.处理配置入参
    * 创建单独线程一直运行(整个进程不主动退出)
    * 当request结果响应后处理 每个结果一次

  * 限制爬取范围

    * URL去重 (get参数变化处理)
    * URL最大数限制
    * URL域范围限制
    * URL爬取深度限制
    * 协议限制,仅支持http

请求模块
--------

* request 基于多线程模型

  * 包裹urllib2进行获取页面内容
  * 由于网络情况这里可能发生卡死
  * 考虑多处超时，超量 第一版本不实现
  * 考虑登录 第一版本不实现
  * 主要处理urldata模型

存储模块
--------
* storage  
> 写数据库模块 


过滤模块
--------

* filter 
> 需要导入，有顺序要求，如果需要参数由调用方的收集或存储模块提供

  * collect.filter 收集模块的过滤器
  * storage.filter 存储模块的过滤器
  * common.filter 通用的过滤器

引擎模块
--------
* engine 引擎模块

  * 负责协调各个模块工作

自动测试脚本
------------

* test 
  * 第一版本应该会有少量关键用例


其他关键目录
-----------

* model 模型数据

  * urldata 关键数据模型

* conf 配置目录

  * 爬虫运行必须的配置


* util 工具函数目录

  * 处理日志参数
  * 命令行参数
  * yaml配置文件解析
  * Thread的基本封装
  * url一些正则校验

* steal 命令行入口

  * 直接运行的程序
